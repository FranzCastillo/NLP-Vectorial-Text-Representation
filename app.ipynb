{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57ph7y23-KX6"
   },
   "source": [
    "# Laboratorio 5 - Representaciones Vectoriales de Texto\n",
    "Francisco Castillo - 21562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRKP32JD-QoX"
   },
   "source": [
    "## 1. Preprocesamiento del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rpd2PcM-Cllm"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J9bY0fG0_QKI"
   },
   "outputs": [],
   "source": [
    "categories=['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'rec.autos']\n",
    "news = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=21562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qj6oS7DP_ft2"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove sequences of repeated characters or potential artifacts like 'outofcontrolgif' and the subsequent random strings\n",
    "    text = re.sub(r'\\b\\w*outofcontrolgif\\w*\\b', '', text) # Remove the specific 'outofcontrolgif' word and any attached characters\n",
    "    text = re.sub(r'\\b[a-z]{15,}\\b', '', text) # Remove words that are 15 or more characters long and consist only of lowercase letters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q8r_8cXzAhWV"
   },
   "outputs": [],
   "source": [
    "preprocessed_news = [preprocess_text(doc) for doc in news.data]\n",
    "tokenized_news = [nltk.word_tokenize(doc) for doc in preprocessed_news]\n",
    "\n",
    "# Remove tokens starting with \"begin\"\n",
    "tokenized_news = [[word for word in doc if not word.startswith('begin')] for doc in tokenized_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YLe6RjkAkzr",
    "outputId": "528a1c00-9134-4830-c496-cb7e5964d3bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'say', 'you', 'and', 'nick', 'go', 'somewhere', 'else', 'with', 'this', 'shool', 'yard', 'crap']\n",
      "['in', 'europe', 'you', 'can', 'buy', 'a', 'ix', 'with', 'computer', 'controlled', 'diffs', 'rather', 'than', 'the', 'horrid', 'viscous', 'coupled', 'ones', 'of', 'the', 'outgoing', 'ix']\n",
      "['i', 'dont', 'think', 'weve', 'got', 'a', 'conspiracy', 'on', 'our', 'hands', 'or', 'anything', 'vaugely', 'similar', 'i', 'do', 'think', 'that', 'the', 'feds', 'showed', 'a', 'distinct', 'lack', 'of', 'both', 'intelligence', 'and', 'disregard', 'for', 'others', 'safety', 'throughout', 'this', 'whole', 'mess', 'i', 'do', 'think', 'the', 'fbi', 'and', 'the', 'batf', 'screwed', 'up', 'big', 'what', 'made', 'me', 'really', 'concerned', 'was', 'fbi', 'director', 'william', 'sessions', 'being', 'on', 'cnn', 'engaging', 'in', 'what', 'could', 'only', 'be', 'called', 'spin', 'control', 'before', 'the', 'place', 'had', 'even', 'cooled', 'down', 'evertyhing', 'had', 'literally', 'blown', 'up', 'in', 'their', 'faces', 'and', 'i', 'felt', 'there', 'had', 'to', 'be', 'something', 'more', 'important', 'he', 'should', 'have', 'been', 'doing']\n",
      "['mfehaeqkkl', 'mgfqqloozfemdj', 'lzylluke', 'end']\n",
      "['for', 'starters', 'they', 'could', 'have', 'gone', 'on', 'waiting', 'and', 'negotiating', 'the', 'davidians', 'werent', 'going', 'anywhere', 'and', 'their', 'supplies', 'had', 'to', 'be', 'limited', 'large', 'perhaps', 'but', 'limited', 'if', 'they', 'had', 'simply', 'fired', 'the', 'compound', 'by', 'themselves', 'without', 'govt', 'tanks', 'smashing', 'down', 'their', 'walls', 'then', 'at', 'least', 'the', 'govt', 'would', 'not', 'be', 'guilty', 'of', 'having', 'again', 'used', 'an', 'inappropriate', 'level', 'of', 'force', 'and', 'would', 'have', 'been', 'able', 'to', 'use', 'the', 'meantime', 'to', 'continue', 'to', 'pressure', 'and', 'negotiate', 'no', 'they', 'would', 'not', 'have', 'looked', 'good', 'on', 'the', 'news', 'in', 'six', 'months', 'or', 'a', 'year', 'but', 'they', 'sure', 'as', 'hell', 'dont', 'look', 'very', 'good', 'now', 'larry', 'smith', 'smithctroncom', 'no', 'i', 'dont', 'speak', 'for', 'cabletron', 'need', 'you', 'ask']\n",
      "['is', 'this', 'a', 'joke']\n",
      "['hi', 'maybe', 'someone', 'can', 'help', 'me', 'here', 'i', 'am', 'looking', 'to', 'buy', 'this', 'nissan', 'maxima', 'gxe', 'for', 'cdn', 'right', 'now', 'the', 'car', 'has', 'km', 'or', 'about', 'miles', 'on', 'it', 'a', 'typical', 'mileage', 'for', 'cars', 'seem', 'to', 'be', 'about', 'km', 'or', 'about', 'k', 'mi', 'the', 'seller', 'just', 'informed', 'me', 'that', 'when', 'he', 'brought', 'the', 'car', 'in', 'for', 'certification', 'he', 'was', 'told', 'that', 'the', 'front', 'break', 'pads', 'and', 'the', 'exhausts', 'had', 'to', 'be', 'replaced', 'to', 'meet', 'the', 'legal', 'standards', 'he', 'said', 'he', 'will', 'replace', 'the', 'components', 'before', 'selling', 'the', 'car', 'to', 'me', 'being', 'copmletely', 'ignorant', 'to', 'the', 'technical', 'stuff', 'on', 'cars', 'i', 'dont', 'know', 'what', 'this', 'could', 'mean', 'is', 'k', 'km', 'about', 'the', 'time', 'typical', 'for', 'replacing', 'the', 'above', 'mentioned', 'items', 'or', 'is', 'this', 'an', 'indication', 'that', 'the', 'car', 'was', 'abused', 'would', 'other', 'things', 'break', 'down', 'or', 'have', 'to', 'be', 'replaced', 'soon', 'the', 'seller', 'told', 'me', 'that', 'he', 'used', 'the', 'car', 'on', 'the', 'highway', 'a', 'lot', 'but', 'i', 'dont', 'know', 'how', 'to', 'verify', 'this', 'ive', 'seen', 'the', 'paint', 'chipped', 'away', 'in', 'tiny', 'dots', 'in', 'the', 'front', 'edge', 'of', 'the', 'hood', 'though', 'although', 'the', 'maxima', 'is', 'an', 'excellent', 'car', 'and', 'the', 'car', 'is', 'very', 'clean', 'and', 'well', 'kept', 'its', 'currently', 'out', 'of', 'warranty', 'a', 'similarly', 'priced', 'accord', 'with', 'k', 'km', 'will', 'have', 'years', 'or', 'k', 'km', 'worth', 'of', 'warranty', 'left', 'and', 'i', 'dont', 'want', 'to', 'worry', 'about', 'paying', 'for', 'any', 'repair', 'bills', 'but', 'i', 'also', 'need', 'a', 'car', 'for', 'people', 'when', 'will', 'the', 'new', 'maxima', 'come', 'out', 'by', 'the', 'way', 'i', 'would', 'very', 'much', 'appreciate', 'your', 'input', 'in', 'this', 'please', 'reply', 'by', 'email', 'preferred', 'or', 'post', 'in', 'this', 'newsgroup', 'thanks', 'ryan']\n",
      "['watch', 'the', 'videotape', 'carefully', 'the', 'cnn', 'coverage', 'was', 'fairly', 'decisive', 'the', 'first', 'fire', 'starts', 'in', 'the', 'tower', 'this', 'is', 'three', 'storeys', 'high', 'and', 'there', 'is', 'a', 'flag', 'to', 'the', 'right', 'of', 'it', 'on', 'the', 'picture', 'the', 'second', 'fire', 'starts', 'in', 'another', 'tower', 'which', 'is', 'similar', 'to', 'the', 'first', 'only', 'two', 'storeys', 'high', 'the', 'flag', 'is', 'on', 'the', 'left', 'in', 'the', 'camera', 'picture', 'that', 'shows', 'this', 'fire', 'starting', 'thus', 'the', 'camera', 'pictures', 'cleraly', 'show', 'the', 'fire', 'starting', 'at', 'two', 'separate', 'locations', 'the', 'fbi', 'report', 'a', 'third', 'i', 'was', 'not', 'able', 'to', 'verify', 'it', 'from', 'the', 'videotape', 'however', 'someone', 'else', 'identified', 'a', 'fire', 'shown', 'to', 'be', 'starting', 'behind', 'the', 'small', 'tower', 'in', 'the', 'second', 'flag', 'on', 'left', 'camera', 'angle', 'the', 'flames', 'coming', 'out', 'of', 'the', 'building', 'are', 'yelloworange', 'this', 'is', 'the', 'normal', 'colour', 'for', 'carbon', 'compounds', 'burning', 'the', 'flames', 'were', 'those', 'of', 'a', 'solid', 'or', 'confined', 'liquid', 'burning', 'not', 'of', 'a', 'gas', 'exploding', 'the', 'explosion', 'that', 'occurs', 'mid', 'way', 'along', 'the', 'building', 'is', 'certainly', 'not', 'an', 'explosive', 'though', 'the', 'cloud', 'itself', 'is', 'on', 'fire', 'this', 'would', 'seem', 'to', 'be', 'most', 'likely', 'to', 'be', 'some', 'sort', 'of', 'fuel', 'oil', 'store', 'exploding', 'rather', 'than', 'the', 'explosion', 'of', 'a', 'magazine', 'depends', 'entirely', 'on', 'how', 'they', 'were', 'distributed', 'you', 'would', 'not', 'be', 'able', 'to', 'identify', 'ammunition', 'rounds', 'going', 'off', 'from', 'video', 'camera', 'coverage', 'from', 'a', 'mile', 'away', 'if', 'and', 'when', 'the', 'fbi', 'release', 'pictures', 'from', 'cmeras', 'on', 'the', 'armoured', 'vehicles', 'which', 'presumably', 'exist', 'it', 'might', 'be', 'possible', 'to', 'get', 'a', 'clearer', 'picture', 'if', 'anyone', 'expects', 'to', 'see', 'explosions', 'hollywood', 'style', 'aka', 'rambo', 'movies', 'then', 'remember', 'that', 'in', 'real', 'life', 'cars', 'do', 'not', 'burst', 'into', 'flames', 'when', 'going', 'over', 'cliffs', 'just', 'about', 'the', 'most', 'you', 'could', 'expect', 'would', 'be', 'to', 'see', 'the', 'grenades', 'going', 'off', 'since', 'the', 'building', 'was', 'designed', 'to', 'be', 'blast', 'proof', 'to', 'some', 'extent', 'it', 'would', 'be', 'difficult', 'to', 'distinguish', 'the', 'grenades', 'going', 'off', 'from', 'the', 'collapse', 'of', 'the', 'building', 'due', 'to', 'the', 'fire', 'paranoia', 'you', 'wouldnt', 'beleive', 'the', 'fbi', 'if', 'they', 'showed', 'you', 'a', 'picture', 'of', 'koresh', 'himself', 'setting', 'light', 'to', 'the', 'place', 'your', 'mindset', 'is', 'such', 'that', 'you', 'are', 'simply', 'unable', 'to', 'accept', 'as', 'true', 'anything', 'that', 'might', 'suggest', 'that', 'a', 'group', 'of', 'heavily', 'armed', 'weapons', 'fanatics', 'might', 'indeed', 'be', 'in', 'the', 'wrong', 'the', 'gun', 'lobby', 'cant', 'accept', 'that', 'the', 'bd', 'set', 'light', 'to', 'the', 'place', 'because', 'that', 'would', 'mean', 'that', 'koreh', 'had', 'murdered', 'children', 'that', 'would', 'mean', 'that', 'their', 'taking', 'his', 'account', 'of', 'the', 'murder', 'of', 'batf', 'agents', 'would', 'be', 'even', 'less', 'credible', 'than', 'it', 'was', 'to', 'start', 'with', 'koresh', 'had', 'days', 'to', 'come', 'out', 'with', 'his', 'hands', 'up', 'and', 'face', 'a', 'fair', 'trial', 'instead', 'he', 'ordered', 'the', 'murder', 'of', 'everyone', 'in', 'the', 'place']\n",
      "[]\n",
      "['my', 'wife', 'rarely', 'carries', 'a', 'purse', 'so', 'all', 'of', 'her', 'crap', 'ends', 'up', 'in', 'my', 'pockets']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenized_news[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoONGIruBUnb"
   },
   "source": [
    "En el preprocesamiento eliminamos lo que parecen ser diferentes gifs e imagenes que contienen cadenas de texto \"aleatorias\" y pueden alterar el desempeño de la vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1f1178a5"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "tokenized_news_cleaned = [remove_stopwords_and_lemmatize(doc) for doc in tokenized_news]\n",
    "\n",
    "# Remove empty documents and their corresponding labels\n",
    "non_empty_indices = [i for i, doc in enumerate(tokenized_news_cleaned) if doc]\n",
    "tokenized_news_cleaned = [tokenized_news_cleaned[i] for i in non_empty_indices]\n",
    "cleaned_target = [news.target[i] for i in non_empty_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0b93a93",
    "outputId": "cdc3ef4b-3f59-4e47-9b68-0a8ecd591aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'nick', 'go', 'somewhere', 'else', 'shool', 'yard', 'crap']\n",
      "['europe', 'buy', 'ix', 'computer', 'controlled', 'diffs', 'rather', 'horrid', 'viscous', 'coupled', 'one', 'outgoing', 'ix']\n",
      "['dont', 'think', 'weve', 'got', 'conspiracy', 'hand', 'anything', 'vaugely', 'similar', 'think', 'fed', 'showed', 'distinct', 'lack', 'intelligence', 'disregard', 'others', 'safety', 'throughout', 'whole', 'mess', 'think', 'fbi', 'batf', 'screwed', 'big', 'made', 'really', 'concerned', 'fbi', 'director', 'william', 'session', 'cnn', 'engaging', 'could', 'called', 'spin', 'control', 'place', 'even', 'cooled', 'evertyhing', 'literally', 'blown', 'face', 'felt', 'something', 'important']\n",
      "['mfehaeqkkl', 'mgfqqloozfemdj', 'lzylluke', 'end']\n",
      "['starter', 'could', 'gone', 'waiting', 'negotiating', 'davidians', 'werent', 'going', 'anywhere', 'supply', 'limited', 'large', 'perhaps', 'limited', 'simply', 'fired', 'compound', 'without', 'govt', 'tank', 'smashing', 'wall', 'least', 'govt', 'would', 'guilty', 'used', 'inappropriate', 'level', 'force', 'would', 'able', 'use', 'meantime', 'continue', 'pressure', 'negotiate', 'would', 'looked', 'good', 'news', 'six', 'month', 'year', 'sure', 'hell', 'dont', 'look', 'good', 'larry', 'smith', 'smithctroncom', 'dont', 'speak', 'cabletron', 'need', 'ask']\n",
      "['joke']\n",
      "['hi', 'maybe', 'someone', 'help', 'looking', 'buy', 'nissan', 'maximum', 'gxe', 'cdn', 'right', 'car', 'km', 'mile', 'typical', 'mileage', 'car', 'seem', 'km', 'k', 'mi', 'seller', 'informed', 'brought', 'car', 'certification', 'told', 'front', 'break', 'pad', 'exhaust', 'replaced', 'meet', 'legal', 'standard', 'said', 'replace', 'component', 'selling', 'car', 'copmletely', 'ignorant', 'technical', 'stuff', 'car', 'dont', 'know', 'could', 'mean', 'k', 'km', 'time', 'typical', 'replacing', 'mentioned', 'item', 'indication', 'car', 'abused', 'would', 'thing', 'break', 'replaced', 'soon', 'seller', 'told', 'used', 'car', 'highway', 'lot', 'dont', 'know', 'verify', 'ive', 'seen', 'paint', 'chipped', 'away', 'tiny', 'dot', 'front', 'edge', 'hood', 'though', 'although', 'maximum', 'excellent', 'car', 'car', 'clean', 'well', 'kept', 'currently', 'warranty', 'similarly', 'priced', 'accord', 'k', 'km', 'year', 'k', 'km', 'worth', 'warranty', 'left', 'dont', 'want', 'worry', 'paying', 'repair', 'bill', 'also', 'need', 'car', 'people', 'new', 'maximum', 'come', 'way', 'would', 'much', 'appreciate', 'input', 'please', 'reply', 'email', 'preferred', 'post', 'newsgroup', 'thanks', 'ryan']\n",
      "['watch', 'videotape', 'carefully', 'cnn', 'coverage', 'fairly', 'decisive', 'first', 'fire', 'start', 'tower', 'three', 'storey', 'high', 'flag', 'right', 'picture', 'second', 'fire', 'start', 'another', 'tower', 'similar', 'first', 'two', 'storey', 'high', 'flag', 'left', 'camera', 'picture', 'show', 'fire', 'starting', 'thus', 'camera', 'picture', 'cleraly', 'show', 'fire', 'starting', 'two', 'separate', 'location', 'fbi', 'report', 'third', 'able', 'verify', 'videotape', 'however', 'someone', 'else', 'identified', 'fire', 'shown', 'starting', 'behind', 'small', 'tower', 'second', 'flag', 'left', 'camera', 'angle', 'flame', 'coming', 'building', 'yelloworange', 'normal', 'colour', 'carbon', 'compound', 'burning', 'flame', 'solid', 'confined', 'liquid', 'burning', 'gas', 'exploding', 'explosion', 'occurs', 'mid', 'way', 'along', 'building', 'certainly', 'explosive', 'though', 'cloud', 'fire', 'would', 'seem', 'likely', 'sort', 'fuel', 'oil', 'store', 'exploding', 'rather', 'explosion', 'magazine', 'depends', 'entirely', 'distributed', 'would', 'able', 'identify', 'ammunition', 'round', 'going', 'video', 'camera', 'coverage', 'mile', 'away', 'fbi', 'release', 'picture', 'cmeras', 'armoured', 'vehicle', 'presumably', 'exist', 'might', 'possible', 'get', 'clearer', 'picture', 'anyone', 'expects', 'see', 'explosion', 'hollywood', 'style', 'aka', 'rambo', 'movie', 'remember', 'real', 'life', 'car', 'burst', 'flame', 'going', 'cliff', 'could', 'expect', 'would', 'see', 'grenade', 'going', 'since', 'building', 'designed', 'blast', 'proof', 'extent', 'would', 'difficult', 'distinguish', 'grenade', 'going', 'collapse', 'building', 'due', 'fire', 'paranoia', 'wouldnt', 'beleive', 'fbi', 'showed', 'picture', 'koresh', 'setting', 'light', 'place', 'mindset', 'simply', 'unable', 'accept', 'true', 'anything', 'might', 'suggest', 'group', 'heavily', 'armed', 'weapon', 'fanatic', 'might', 'indeed', 'wrong', 'gun', 'lobby', 'cant', 'accept', 'bd', 'set', 'light', 'place', 'would', 'mean', 'koreh', 'murdered', 'child', 'would', 'mean', 'taking', 'account', 'murder', 'batf', 'agent', 'would', 'even', 'less', 'credible', 'start', 'koresh', 'day', 'come', 'hand', 'face', 'fair', 'trial', 'instead', 'ordered', 'murder', 'everyone', 'place']\n",
      "['wife', 'rarely', 'carry', 'purse', 'crap', 'end', 'pocket']\n",
      "['oh', 'ok', 'wondering', 'real', 'expert', 'weapon', 'wondering', 'would', 'job']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenized_news_cleaned[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FTCYHRHCSwi"
   },
   "source": [
    "También, dado que no generaremos texto procedemos a lematizar y eliminar las stopwords para obtener mejor precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93e3d00a",
    "outputId": "fa9ae93f-f839-42c5-b73d-929045f1af49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2616\n",
      "Testing set size: 873\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_news_cleaned, cleaned_target, test_size=0.25, random_state=21562)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyGPeUmzDjoB"
   },
   "source": [
    "## 2. Construcción de representación TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "341d8186"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c078430d",
    "outputId": "60bec0d9-92a0-4583-9a4b-fbb6d3e62660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTF-IDF Train:  (2616, 25760)\n",
      "TF-IDF Test:  (873, 25760)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform([\" \".join(doc) for doc in X_train])\n",
    "X_test_tfidf = tfidf_vectorizer.transform([\" \".join(doc) for doc in X_test])\n",
    "\n",
    "print(\"TTF-IDF Train: \", X_train_tfidf.shape)\n",
    "print(\"TF-IDF Test: \", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KlQotUM5F177"
   },
   "outputs": [],
   "source": [
    "def get_top_tfidf_words(tfidf_matrix_row, feature_names, n=5):\n",
    "    # Get the indices of the top n TF-IDF scores\n",
    "    top_n_indices = tfidf_matrix_row.argsort()[-n:][::-1]\n",
    "    # Get the corresponding words and scores\n",
    "    top_n_words = [(feature_names[i], tfidf_matrix_row[i]) for i in top_n_indices]\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb9860a4",
    "outputId": "4695122c-c666-41fb-b229-05983809fdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "ohio: 0.2662\n",
      "frank: 0.2662\n",
      "bad: 0.2527\n",
      "regardless: 0.2220\n",
      "trial: 0.2107\n",
      "\n",
      "--- Document 2 ---\n",
      "serb: 0.6182\n",
      "muslim: 0.3360\n",
      "bosnian: 0.2662\n",
      "serbia: 0.2124\n",
      "refused: 0.1876\n",
      "\n",
      "--- Document 3 ---\n",
      "being: 0.2249\n",
      "automobile: 0.2039\n",
      "honest: 0.2028\n",
      "value: 0.1680\n",
      "criminal: 0.1600\n",
      "\n",
      "--- Document 4 ---\n",
      "gun: 0.3145\n",
      "shoot: 0.2573\n",
      "sw: 0.2199\n",
      "training: 0.2163\n",
      "revolver: 0.1899\n",
      "\n",
      "--- Document 5 ---\n",
      "slip: 0.4499\n",
      "tranny: 0.2771\n",
      "gear: 0.2504\n",
      "usually: 0.2110\n",
      "speed: 0.2104\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    tfidf_row = X_train_tfidf[i].toarray()[0]\n",
    "    top_words = get_top_tfidf_words(tfidf_row, feature_names)\n",
    "    for word, score in top_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ182-FpIEgf"
   },
   "source": [
    "### Palabras con mayor peso\n",
    "Las palabras con más peso (los scores TF-IDF más altos) en un documento particular son aquellas que son importantes dentro de ese documento y, al mismo tiempo, no son muy comunes en el resto del corpus. Esto significa que esas palabras son las que mejor representan o distinguen el tema o contenido específico de ese documento en comparación con otros documentos. Por ejemplo, en el \"Documento 2\", palabras como \"serb\", \"muslim\", \"bosnian\" y \"serbia\" tienen altos pesos, lo que sugiere que este documento trata sobre el conflicto en los Balcanes. De manera similar, en el \"Documento 4\", términos como \"gun\", \"shoot\", \"training\" y \"revolver\" indican claramente que el documento está relacionado con el tema de las armas. En resumen, estos altos pesos nos señalan las palabras clave que son muy relevantes y distintivas para cada documento particular.\n",
    "\n",
    "### Limitaciones semánticas de TF-IDF\n",
    "\n",
    "TF-IDF se basa únicamente en la frecuencia y rareza de las palabras, sin considerar su significado o el contexto en el que aparecen. Al tratar los documentos como una \"bolsa de palabras\", ignora el orden y las relaciones entre ellas, lo que impide capturar sinónimos, polisemia o relaciones semánticas más profundas. Esto significa que, aunque identifica palabras clave relevantes, no comprende el sentido completo ni las conexiones entre las ideas en un texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4NasabwPCeX"
   },
   "source": [
    "## 3. Construcción de Representación PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "087029e9"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def build_cooccurrence_matrix(tokenized_docs, window_size):\n",
    "    word_counts = defaultdict(int)\n",
    "    cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        for i, target_word in enumerate(doc):\n",
    "            word_counts[target_word] += 1\n",
    "            start_index = max(0, i - window_size)\n",
    "            end_index = min(len(doc), i + window_size + 1)\n",
    "\n",
    "            for j in range(start_index, end_index):\n",
    "                if i != j:\n",
    "                    context_word = doc[j]\n",
    "                    cooccurrence_counts[target_word][context_word] += 1\n",
    "\n",
    "    vocabulary = list(word_counts.keys())\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "    # Create a sparse matrix for co-occurrence counts\n",
    "    cooccurrence_matrix = lil_matrix((len(vocabulary), len(vocabulary)), dtype=int)\n",
    "\n",
    "    for target_word, context_data in cooccurrence_counts.items():\n",
    "        target_index = word_to_index[target_word]\n",
    "        for context_word, count in context_data.items():\n",
    "            context_index = word_to_index[context_word]\n",
    "            cooccurrence_matrix[target_index, context_index] = count\n",
    "\n",
    "    return cooccurrence_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2ec056cb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_ppmi(cooccurrence_matrix, vocabulary):\n",
    "    total_pairs = cooccurrence_matrix.sum()\n",
    "    word_sums = cooccurrence_matrix.sum(axis=1).A1  # Sum of each row (target words)\n",
    "    context_sums = cooccurrence_matrix.sum(axis=0).A1 # Sum of each column (context words)\n",
    "\n",
    "    ppmi_matrix = lil_matrix(cooccurrence_matrix.shape, dtype=float)\n",
    "\n",
    "    rows, cols = cooccurrence_matrix.nonzero() # Get indices of non-zero elements\n",
    "\n",
    "    for row, col in zip(rows, cols):\n",
    "        cooc_count = cooccurrence_matrix[row, col]\n",
    "        p_target_context = cooc_count / total_pairs\n",
    "        p_target = word_sums[row] / total_pairs\n",
    "        p_context = context_sums[col] / total_pairs\n",
    "\n",
    "        # Handle cases to avoid division by zero or log of zero\n",
    "        if p_target > 0 and p_context > 0:\n",
    "            pmi = np.log2(p_target_context / (p_target * p_context))\n",
    "            ppmi = max(0, pmi)\n",
    "            ppmi_matrix[row, col] = ppmi\n",
    "        else:\n",
    "            ppmi_matrix[row, col] = 0\n",
    "\n",
    "    return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64e499ce",
    "outputId": "d4f221b5-ff87-4b17-ceee-c015a3e804cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI Matrix Shape: (29942, 29942)\n"
     ]
    }
   ],
   "source": [
    "cooccurrence_matrix, vocabulary = build_cooccurrence_matrix(tokenized_news_cleaned, window_size=4)\n",
    "ppmi_matrix = calculate_ppmi(cooccurrence_matrix, vocabulary)\n",
    "print(\"PPMI Matrix Shape:\", ppmi_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AI0q5cbdTdA"
   },
   "source": [
    "## 4. Construcción de representación Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuGmJztYfsQW"
   },
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r35KLHaiemlQ"
   },
   "outputs": [],
   "source": [
    "vector_size = 100  # Dimension of the word vectors\n",
    "window = 5       # Context window size\n",
    "min_count = 5    # Ignore words with frequency lower than this\n",
    "workers = 4      # Number of CPU cores to use\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyJNOQLyU5b-",
    "outputId": "606d683d-446b-4670-f6d4-71a09bc48eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caqy-Y5Njqlb",
    "outputId": "707778f4-cf13-4fe2-cf67-b12647d056f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3818409, 4240950)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "model.build_vocab(tokenized_news_cleaned)\n",
    "\n",
    "total_examples = model.corpus_count\n",
    "model.train(tokenized_news_cleaned, total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CbRzGM31e5Yl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(doc, model):\n",
    "    word_vectors = []\n",
    "    for word in doc:\n",
    "        # Check if the word exists in the Word2Vec model's vocabulary\n",
    "        if word in model.wv.key_to_index:\n",
    "            # If the word is in the vocabulary, retrieve its vector using model.wv[word] and append it to the list of word vectors for the document.\n",
    "            word_vectors.append(model.wv[word])\n",
    "\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uw4isBzlnZP",
    "outputId": "9830208c-53c8-4aae-80d4-01eb779d197f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3489, 100)\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = [document_vector(doc, model) for doc in tokenized_news_cleaned]\n",
    "document_embeddings = np.array(document_embeddings)\n",
    "\n",
    "print(document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YU7osaJofC69",
    "outputId": "32cd8a2c-823b-40fb-9a5b-d192ee387de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Words Most Similar ---\n",
      "\n",
      "Words similar to 'jew':\n",
      "  christian: 0.8579\n",
      "  wwii: 0.8252\n",
      "  jewish: 0.8184\n",
      "  holocaust: 0.7992\n",
      "  gaza: 0.7969\n",
      "\n",
      "Words similar to 'christian':\n",
      "  wwii: 0.9240\n",
      "  extermination: 0.9200\n",
      "  croatia: 0.9177\n",
      "  islam: 0.9113\n",
      "  hatred: 0.9106\n",
      "\n",
      "Words similar to 'muslim':\n",
      "  croat: 0.9609\n",
      "  bosnian: 0.9478\n",
      "  serb: 0.9012\n",
      "  ethnically: 0.8858\n",
      "  exterminated: 0.8799\n",
      "\n",
      "Words similar to 'police':\n",
      "  officer: 0.8381\n",
      "  agent: 0.8106\n",
      "  charge: 0.7711\n",
      "  local: 0.7474\n",
      "  sheriff: 0.7438\n",
      "\n",
      "--- Word Pair Similarities ---\n",
      "Similarity between 'gun' and 'shoot': 0.5484\n",
      "Similarity between 'car' and 'wheel': 0.8818\n",
      "Similarity between 'politics' and 'mideast': 0.8180\n"
     ]
    }
   ],
   "source": [
    "words_to_explore = ['jew', 'christian', 'muslim', 'police']\n",
    "\n",
    "print(\"--- Words Most Similar ---\")\n",
    "for word in words_to_explore:\n",
    "    if word in model.wv:\n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        similar_words = model.wv.most_similar(word, topn=5)\n",
    "        for sim_word, score in similar_words:\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary.\")\n",
    "\n",
    "\n",
    "# Calculate similarity between word pairs\n",
    "word_pairs = [('gun', 'shoot'), ('car', 'wheel'), ('politics', 'mideast')]\n",
    "\n",
    "print(\"\\n--- Word Pair Similarities ---\")\n",
    "for word1, word2 in word_pairs:\n",
    "    if word1 in model.wv and word2 in model.wv:\n",
    "        similarity_score = model.wv.similarity(word1, word2)\n",
    "        print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"One or both words ('{word1}', '{word2}') not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaKpqgcLmYd9"
   },
   "source": [
    "## 5. Evaluación Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "326b2fa5"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kph2dYQYhDgo"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WQ9F8bRSq0tu"
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, param_grid):\n",
    "    logistic_regression = LogisticRegression(max_iter=1000)\n",
    "    grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, scoring='precision_weighted', cv=3) # Using 'precision_weighted' for multiclass\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    precision_score = grid_search.score(X_test, y_test)\n",
    "    print(f\"Precision: {precision_score:.4f}\")\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfZYjNjm8PpT",
    "outputId": "84e5fea0-a9f5-4de8-a3c2-89b93269a851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  TF-IDF ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "Precision: 0.8470\n"
     ]
    }
   ],
   "source": [
    "print(\"---  TF-IDF ---\")\n",
    "grid_search_tfidf = train_model(X_train_tfidf, y_train, X_test_tfidf, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "U4eY6Xy5rAyl"
   },
   "outputs": [],
   "source": [
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "def document_vector_ppmi(doc, ppmi_matrix, word_to_index):\n",
    "    word_vectors = []\n",
    "    for word in doc:\n",
    "        if word in word_to_index:\n",
    "            word_index = word_to_index[word]\n",
    "            word_vector = ppmi_matrix[word_index, :].toarray().flatten()\n",
    "            word_vectors.append(word_vector)\n",
    "\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(ppmi_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "B76na4Oy8nMl"
   },
   "outputs": [],
   "source": [
    "ppmi_document_representations = [document_vector_ppmi(doc, ppmi_matrix, word_to_index) for doc in tokenized_news_cleaned]\n",
    "ppmi_document_representations = np.array(ppmi_document_representations)\n",
    "X_train_ppmi, X_test_ppmi, y_train_check, y_test_check = train_test_split(\n",
    "    ppmi_document_representations,\n",
    "    cleaned_target,\n",
    "    test_size=0.25,\n",
    "    random_state=21562\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "804b70d3",
    "outputId": "0d6490d2-c46a-4b56-b9ba-af70a19b0d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression with PPMI ---\n",
      "Best parameters found:\n",
      "{'C': 0.01, 'penalty': 'l2'}\n",
      "Precision: 0.8344\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training Logistic Regression with PPMI ---\")\n",
    "grid_search_ppmi = train_model(X_train_ppmi, y_train, X_test_ppmi, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeabf201",
    "outputId": "ac8a256a-a7f2-4620-b802-2bda11a94944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Word2Vec ---\n",
      "Best parameters found:\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "Precision: 0.7884\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v, X_test_w2v, y_train_check_w2v, y_test_check_w2v = train_test_split(\n",
    "    document_embeddings,\n",
    "    cleaned_target,\n",
    "    test_size=0.25,\n",
    "    random_state=21562\n",
    ")\n",
    "\n",
    "print(\"\\n--- Word2Vec ---\")\n",
    "grid_search_w2v = train_model(X_train_w2v, y_train, X_test_w2v, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c21f7da8",
    "outputId": "39fcc789-ecd0-4a43-d146-cf2793121105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Precision Comparison ---\n",
      "TF-IDF Model Best Precision: 0.8397\n",
      "PPMI Model Best Precision: 0.8186\n",
      "Word2Vec Model Best Precision: 0.7766\n"
     ]
    }
   ],
   "source": [
    "best_precision_tfidf = grid_search_tfidf.best_score_\n",
    "best_precision_ppmi = grid_search_ppmi.best_score_\n",
    "best_precision_w2v = grid_search_w2v.best_score_\n",
    "\n",
    "print(\"\\n--- Model Precision Comparison ---\")\n",
    "print(f\"TF-IDF Model Best Precision: {best_precision_tfidf:.4f}\")\n",
    "print(f\"PPMI Model Best Precision: {best_precision_ppmi:.4f}\")\n",
    "print(f\"Word2Vec Model Best Precision: {best_precision_w2v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vq3m1dUov2y"
   },
   "source": [
    "La representación TF-IDF logró el mejor rendimiento (0.8397), seguida por PPMI (0.8186) y finalmente Word2Vec (0.7766). Este orden podría parecer inicialmente sorprendente, ya que Word2Vec y PPMI tienen la capacidad de capturar relaciones semánticas que TF-IDF no. Sin embargo, para una tarea de clasificación de noticias como esta, la importancia distintiva de las palabras clave (capturada efectivamente por TF-IDF) puede ser un factor más determinante que las relaciones semánticas finas. Es posible que las categorías de noticias seleccionadas estén bien diferenciadas por la presencia de términos específicos que aparecen con alta frecuencia en una categoría y baja en otras. Además, el rendimiento de Word2Vec puede depender mucho de la calidad de los embeddings entrenados con un corpus particular y del método utilizado para agregarlos a nivel de documento (como el promedio de vectores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EgtyDgQmb2S"
   },
   "source": [
    "## 6. Discusión Final\n",
    "### Cómo cada representación captura (o no) relaciones semánticas\n",
    "TF-IDF no captura directamente relaciones semánticas, ya que se enfoca en la importancia de una palabra dentro de un documento en relación con su frecuencia en todo el corpus, sin considerar el contexto o el significado de las palabras. PPMI captura relaciones semánticas de coexistencia o asociación entre palabras basándose en la frecuencia con la que aparecen juntas dentro de una ventana de contexto; un PPMI alto indica una fuerte asociación. Por otro lado, Word2Vec captura relaciones semánticas más ricas al representar palabras en un espacio vectorial continuo donde palabras con significados similares o que aparecen en contextos similares tienen vectores cercanos; permite capturar relaciones como analogías (\"rey\" - \"hombre\" + \"mujer\" ≈ \"reina\"). Aunque en este caso particular TF-IDF tuvo la mejor precisión, la capacidad de PPMI y Word2Vec para capturar asociaciones y significados a menudo es crucial para tareas que requieren una comprensión semántica más profunda.\n",
    "### ¿En que escenarios es más útil cada técnica?\n",
    "TF-IDF es especialmente útil en escenarios donde la relevancia de un documento se basa en la presencia de palabras clave distintivas y no tanto en las relaciones semánticas complejas, como en sistemas de recuperación de información, motores de búsqueda simples o filtrado de spam. PPMI es valioso para tareas que analizan la co-ocurrencia de palabras para entender asociaciones, construir diccionarios de sinónimos o realizar análisis de sentimientos basados en palabras adyacentes. Word2Vec es muy efectivo en aplicaciones donde es fundamental capturar el significado y las relaciones semánticas entre palabras y documentos, como en la traducción automática, recomendación de contenido, análisis de similitud semántica de documentos, o como capa de entrada para modelos de deep learning.\n",
    "\n",
    "### ¿Cuáles son las limitaciones prácticas (memoria, tiempo de cómputo, interoperabilidad)?\n",
    "Las representaciones TF-IDF y PPMI pueden resultar en matrices muy grandes y dispersas (con muchos ceros) para vocabularios extensos, lo que puede consumir mucha memoria y ser ineficiente computacionalmente para ciertas operaciones. El cálculo de la matriz de co-ocurrencia para PPMI puede ser costoso en tiempo, especialmente con ventanas de contexto amplias. Word2Vec produce vectores densos de menor dimensión, lo que generalmente reduce los requisitos de memoria y hace las operaciones posteriores más rápidas; sin embargo, el entrenamiento del modelo Word2Vec en grandes corpus puede ser computacionalmente intensivo. En cuanto a la interoperabilidad, las matrices TF-IDF y PPMI están ligadas al corpus con el que fueron creadas, mientras que los embeddings de Word2Vec pueden ser pre-entrenados en corpus masivos y reutilizados en diferentes tareas y dominios, ofreciendo mayor interoperabilidad."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
